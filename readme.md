# CD3
Утилита для парсинга html Ютуба и скачивания комментариев к видео.
Переписываю аналогичный кодна Питоне, пока всё подряд накидываю в Мейне, потом порефакторю. В перспективе будет сервис.

## Алгоритм
Сначала получаем html, оттуда берем конфигурацию, часть которой будем использовать в ajax-запросах

находим эндпойнты, характерные для прокрутки и дальнейшей подгрузки, для каждого (они называются continuations) делаем ajax-запрос и парсим ответ. 
Если в ответе находим другие эндпойнты для следующих ajax-запросов, их тоже добавляем в список continuations

## Текущее состояние:
Вроде работает, но количество выданных комментариев не совпадает со счетчиком,который отображает ЮТ над блоклм комментов. При ручном подсчете вышлорасхождение в 1 коммент, так что скорее всего дело в скрытых или удаленных модератором. Для наших целей это хороший результат.

В исходном скрипте не реализована группировка, то есть если коммент вляется ответом, то непонятно, на что конкретно это ответ. 
Строго говоря, есть 2 вида "ответов" - список ответов к первому 

В выводе есть только boolean reply = true и айдишник коммента, который состоит из двух частей. 
Попробуем проанализировать айдишники, надо составить таблицу

|id оригинала               | id ответа                                         | текст для поиска
+---------------------------+---------------------------------------------------+----------------------------------------------
|Ugz25cqFRKgIIVYpzZp4AaABAg |                                                   |Земля плоская! Электричества не существует!
|                           |Ugz25cqFRKgIIVYpzZp4AaABAg.A547Lcuvd9AA548QuyCbBn  |Три слона уже устали
|                           |Ugz25cqFRKgIIVYpzZp4AaABAg.A547Lcuvd9AA549Chp7qJJ  |Настоящий бог Колбас!

|UgwSZa9HUSBkMsloDBh4AaABAg |                                                   |Тргда может бахнем ?
|                           |UgwSZa9HUSBkMsloDBh4AaABAg.A544MKu4LeBA549aCz_x0K  |Может и бахнем , но позже

|UgyhTnBOZUlAKdKPKhV4AaABAg |                                                   |Посмотрим какой это фейк, когда бахнут
|                           |UgyhTnBOZUlAKdKPKhV4AaABAg.A543zZxY-FlA54ErR7cyE2  |Не бахнут,ато фейк расскроется

Ну, тут всё очевидно: root-комментарий имеет короткий id, а ответы на него имеют такой же id, но с добавлением точки и дополнительного айдишника. Напрашивается отличный простой способ отобразить ветки ответов. Добавим в наш вывод поля reply и id, и отсортируем по ним конечный список, для этого напишем анонимный компаратор по двум пропертям - первая часть айдишника , и порядковый номер в изначальном списке выдачи. 

## Улучшение кода
На даннный момент скорость оставляет желать лучшего :/ Ёще бы - гоняем туда-сюда json'ы по нескольку раз. По-хорошему, надо выделить сразу все константы из html и первоначальные данные, оптимизировать поиск по json (особенно ответ на ajax-запрос парсится много раз), потом переписать всё на Мапы, но начнём с другого. Чтобы было удобнее улучшать код, сперва порефакторим. Очень хочется, раз уж пишем новый класс, написать всё сразу правильно и оптимизированно, но надо быть последовательными, и не сломать код, пока не написаны тесты. Так что, скрепя сердце, вооружимся Ctrl+C и Ctrl+V...., ограничась небольшими модификациями.

Вместо статического main сделаем "умный" объект Downloader, и начнем рефакторить с двух концов.

На конце Downloader'а сделаем дата-класс Comment. Поля нам уже известны - пoрядковый номер в нашем алгоритме (который выдает их по новизне, вроде бы), имя автора, текст комментария, isReply и commentId. в итерациях по commentEntityPayload будем добавлять в итоговый лист новые объекты с заполненными полями. На начале - переместим все подготовительные операции в конструктор, да передадим в него произвольный url. Экстрактим методы, сложное пока не трогаем. Переместим кое-что кое-куда. Добавим таймер в main().

28 секундна видео с 500 комментариями, много! будем оптимизировать. 


## Оптимизация обработки ответа
Сразубросается в глаза повторный перебор элементов json, чтобы найти одельно все "reloadContinuationItemsCommand" и "appendContinuationItemsAction". Хорошо бы каждый ajax-ответ парсить только один раз. И вообще этот jsonSearch() надо упразднить, заменив его тремя узкоспециальными методами, которые будут брать данные по заранее известному пути в ответе, вместо того чтобы три и более раз делать поиск по огромному json'у в 15тыс строк. Сделаем отдельный метод, который будет вызываться один раз после того, как придёт ajax-ответ, чтобы его распарсить и наполнить всё что нам надо - actions и mutations. Но на самом деле это не главное. 

Основная проблема производительности в том, как делаются ajax-запросы. Большое кол-во запросов (этого мы изменить не можем) и долгое ожидание ответа очень сильно вилияют перфоманс. Янаписал небольшой колхозный бенчмарк, и вот результаты по выполнению фофч-запросов при разных условиях подключения к интернету (и для сравнения результаты ping до сайта ютуба)


                        |Моб интернет   |wi-fi оптика   |оптика ethernet
    ping youtube.com    |   180-900     |   125         |  на 2мс меньше..
    мин время           |   294         |   171         |  - 
    макс время          |   2052        |   841         |  - 
    сред время          |   536         |   213         |  - 
    кол-во запросов = 111

В общем, надо пойти путём многопоточности запросов, чтобы не ждать пока придет ответ, а к примеру, спустя 50мс отправлять следующий запрос в отдельном потоке, и в нем же обрабатывать ответ, который придет еще спустякакое-то время. а мы уже ктому времени 10 новых тредов создали и 10 запросов отправили,вот так.. Вообще для этого есть webFlux, но чтобы им пользоваться, надо сперва переехать на Спринг. Ничего против я не имею, особенноесли в будущем это будет сервисом.

Видимо, для этого надо еще довольно много порефакторить. Например, вынести запрос с параметрами в отдельный класс, чтобы webFlux создавал его инстансы. Возможно даже начать новый Спринг-бутовый проект?

#### на данный момент я закомментил вызов методов sortComments() и printComments(). 
#### Чтобы скачать комментарии, их надо раскомментировать обратно

